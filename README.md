ğŸŸ¥ Redskull â€“ The All-Seeing Crawler
"You could have the power of the gods!" â€“ But you choose to crawl websites instead.

ğŸ•µï¸ What is Redskull?
Redskull is a TypeScript-powered web crawler that interrogates a site's robots.txt and uncovers all available sitemap.xml URLs.
It organizes discovered links into a tree-like structure, providing a clear view of a site's architecture.

No infinity stones requiredâ€”just pnpm or npm.

ğŸš€ Features
ğŸ”¹ Fetches and parses robots.txt to understand crawl permissions.
ğŸ”¹ Extracts and analyzes all sitemap URLs while respecting robot rules.
ğŸ”¹ Filters outdated links and keeps results relevant.
ğŸ”¹ Visualizes site structure in an easy-to-read tree format.

ğŸ›  Setup
Experimenting with pnpm, so if you donâ€™t have it yet:
npm install -g pnpm@latest-10

ğŸ— Installation & Usage
1ï¸âƒ£ Install dependencies:
'pnpm install'

2ï¸âƒ£ Set the target website in main.ts:
const siteUrl = 'https://your-site.com/'; // Change this to your desired site

3ï¸âƒ£ Highly recommended: Since the output is quite large, save it for later review:
pnpm run dev | tee output.log
(This ensures nothing gets lost in the terminal!)

4ï¸âƒ£ Run the crawler normally if needed:
pnpm run dev

ğŸ•¶ Debugging (Optional)
If you want to debug the script using VS Code, use the provided launch.json configuration.